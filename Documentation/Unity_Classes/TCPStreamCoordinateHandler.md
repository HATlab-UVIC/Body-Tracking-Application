# Class : TCPStreamCoordinateHandler  

## Methods List  

```C#
public static bool TCPDataReadHandler(string);
public static Vector3[] Apply_BodyAlignmentOfset(Vector3[]);
private static Vector3 TrackSpatialMovement();
public static void InitJointCoordinates(Transform)
```  

## Class Description  

The `class : TCPStreamCoordinateHandler` class is a static class that is used for converting the Openpose joint coordinate data from `type : string` to `type : Vector3[]`. This class contains the event handler method for the `TCPServer : event : TCP_Data_Read` event in the local TCP Server class which takes in the coordinate data in string form and converts its to an array of `object : Vector3` objects that can be used for updating our patient model. The class also employs other methods which handle the translational placement of the patient model in the scene.  

## Coordinate Conversion  

The coordinate conversion process occurs in the `method : TCPDataReadHandler()` method. This method takes in a string argument (ie. the string of data returned by Openpose) and seperates the formatted string into individual groups of XYZ values, isolating each point. Once each point is isolated, the individual X, Y, and Z float values can be extracted and stored to a `object : Vector3` object.  

```C#
float SCALE_FACTOR = SCALE_FACTOR_DEFAULT - (OFFSET_FACTOR * SCALE_FACTOR_OFFSET);
// store the x, y, z components as float values
for (int j = 0; j < 3; j++) XYZ_VectorComponents_Floats[j] = float.Parse
                                            (XYZ_VectorComponents_Strings[j]) / SCALE_FACTOR;

// save components as Vector3 and store to body joint coordinate vectors variable
Joint_Vector = new Vector3(XYZ_VectorComponents_Floats[0], XYZ_VectorComponents_Floats[1], 
                            XYZ_VectorComponents_Floats[2]);
```  

Note that while extracting each of the X, Y, and Z float values, they are divided by a scale factor. The reason for scaling the points is because the coordinates Openpose returns are pixel coordinates, coordinates relative to the pixel location of the joint in the image. The points are scaled down to more appropriately match the size of the user. The scale factor is also dynamic. As the user moves closer or farther from the mirror, the distance with which they moved is used to increase or decrease the scale factor. This is done to account for the returned Openpose coordinates being farther apart when the user is closer to the mirror (user is larger in the image), or closer together when farther from the mirror (user is smaller in the image).

Once each individual point has been turned into Vector3 objects, we can group them in an array, thus making a collection of all the joint coordinates we can further operate on before passing the coordinates onto the `class : BodyPositionManager` class.  

> *Note:* The input string can be passed in two formats:  
> 1. `[[[X Y Z]\n  [X Y Z]\n  [X Y Z]\n ... [X Y Z]\n  [X Y Z]]]`  
> 2. `[[[X Y Z][X Y Z][X Y Z]...[X Y Z][X Y Z]]]`  
>  
> Format 1 is the format with which Openpose itself returns its coordinate string, including a new line followed by two spaces. Format 2 is the the output generated by the 3D depth calculation function, returning coordinates with no spaces between the points.  
>  
> **Note:** The above method is designed specifically to handle 25 points of data. If more data points are to be included or if the data format itself is changed, the function of the handler may need to be modified.   

## Patient Game Object Positioning  

Another job of the `class : TCPStreamCoordinateHandler` class is to properly position the patient model in the scene. The `method : Apply_BodyAlignmentOffset()` is used to achieve this.  

Once the `method : TCPDataReadHandler()` has finished creating the `object : Vector3[]` array containing all of the joint vectors, the joint vector coordinates are translated to the position in the scene in front of the user. This is done by calculating an offset value that is used to translate the joint coordinates:  

```C#
public static Vector3[] Apply_BodyAlignmentOffset(Vector3[] JointCoordinate_Vectors)
{
    Vector3 TrackedAlignment_Position = TrackSpatialMovement();
    BodyAlignment_Offset = JointCoordinate_Vectors[0] - TrackedAlignment_Position;

    // ...
}

private static Vector3 TrackSpatialMovement()
{
    Vector3 _alignment = BodyAlignment_Position;

    _alignment.x += BodyPositionManager.HoloLens_Transform_position.x;
    _alignment.y -= BodyPositionManager.HoloLens_Transform_position.y;
    _alignment.z -= BodyPositionManager.HoloLens_Transform_position.z;

    return _alignment;
}
```  

The first step in determining the offset value is calling the `method : TrackSpatialMovement()` method. This method makes use of two important variables:  

|Variable|Description|
|--------|-----------|
|Vector3 BodyAlignment_Position|The transform position vector representing the position of the `GameObject : BodyAlignmentPosition` game object of the unity scene. This position vector represents the starting render position for the patient model to appear in the scene. The initializing body pose applied to the patient model will appear at this position during startup.|
|Vector3 HoloLens_Transform_position|When the application loads, the starting headset or camera world coordinates are (0, 0, 0). This variable represents the translational movement of the headset which can be applied to our offset value to match the translational movement of the user to the movement of the patient model in the scene (ie. the patient model will move in, out, up, down, left, right with the user)|  

Using the `method : TrackSpatialMovement()` method, we are determining the location in the scene with which we want to display the patient model. Once we have defined this location, we need to calculate our offset value. To calculate the offset, we must find the difference between one of our joint coordinates and our alignment location. By applying this offset value to each of the joint coordinates, the joint values will be translated to the alignment position while maintaining their position relative to each other.  

It is also important to note that all of the Y coordinates are flipped, this is due to the nature of the application being used to mirror the users movement back towards them.  

Once the offset has been applied to each of the joint coordinates, we can pass the set of vectors to the `class : BodyPositionManager` class for updating the patient model displayed in the scene.
